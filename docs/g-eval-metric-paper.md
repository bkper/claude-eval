# G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment

**Authors:** Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu  
**Affiliation:** Microsoft Cognitive Services Research  
**Contact:** {yaliu10, iterdan, yicxu, shuowa, ruox, chezhu}@microsoft.com  
**arXiv:** arXiv:2303.16634v3 [cs.CL] 23 May 2023  
**GitHub:** https://github.com/nlpyang/geval

## Abstract

The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-EVAL, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-EVAL with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose analysis on the behavior of LLM-based evaluators, and highlight the potential concern of LLM-based evaluators having a bias towards the LLM-generated texts.

## 1. Introduction

Evaluating the quality of natural language generation systems is a challenging problem even when large language models can generate high-quality and diverse texts that are often indistinguishable from human-written texts (Ouyang et al., 2022). Traditional automatic metrics, such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005), are widely used for NLG evaluation, but they have been shown to have relatively low correlation with human judgments, especially for open-ended generation tasks. Moreover, these metrics require associated reference output, which is costly to collect for new tasks.

Recent studies propose directly using LLMs as reference-free NLG evaluators (Fu et al., 2023; Wang et al., 2023). The idea is to use the LLMs to score the candidate output based on its generation probability without any reference target, under the assumption that the LLMs have learned to assign higher probabilities to high-quality and fluent texts. However, the validity and reliability of using LLMs as NLG evaluators have not been systematically investigated. In addition, meta-evaluations show that these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators (Zhong et al., 2022). Thus, there is a need for a more effective and reliable framework for using LLMs for NLG evaluation.

In this paper, we propose G-EVAL, a framework of using LLMs with chain-of-thoughts (CoT) (Wei et al., 2022) to evaluate the quality of generated texts in a form-filling paradigm. By only feeding the Task Introduction and the Evaluation Criteria as a prompt, we ask LLMs to generate a CoT of detailed Evaluation Steps. Then we use the prompt along with the generated CoT to evaluate the NLG outputs. The evaluator output is formatted as a form. Moreover, the probabilities of the output rating tokens can be used to refine the final metric. We conduct extensive experiments on three meta-evaluation benchmarks of two NLG tasks: text summarization and dialogue generation. The results show that G-EVAL can outperform existing NLG evaluators by a large margin in terms of correlation with human evaluations. Finally, we conduct analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluator having a bias towards the LLM-generated texts.

### Main Contributions

1. **LLM-based metrics generally outperform** reference-based and reference-free baseline metrics in terms of correlation with human quality judgments, especially for open-ended and creative NLG tasks, such as dialogue response generation.

2. **LLM-based metrics are sensitive to the instructions and prompts**, and chain-of-thought can improve the performance of LLM-based evaluators by providing more context and guidance.

3. **LLM-based metrics can provide a more fine-grained continuous score** by re-weighting the discrete scores by their respective token probabilities.

4. **LLM-based metrics have a potential issue of preferring LLM-generated texts** over human-written texts, which may lead to the self-reinforcement of LLMs if LLM-based metrics are used as the reward signal for improving themselves.

## 2. Method

G-EVAL is a prompt-based evaluator with three main components:

1. **A prompt** that contains the definition of the evaluation task and the desired evaluation criteria
2. **A chain-of-thoughts (CoT)** that is a set of intermediate instructions generated by the LLM describing the detailed evaluation steps
3. **A scoring function** that calls LLM and calculates the score based on the probabilities of the return tokens

### Prompt for NLG Evaluation

The prompt is a natural language instruction that defines the evaluation task and the desired evaluation criteria. For example, for text summarization, the prompt can be:

```
You will be given one summary written for a news article. Your task is to rate the summary on one metric.

Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.
```

The prompt should also contain customized evaluation criteria for different NLG tasks, such as coherence, conciseness, or grammar. For example, for evaluating coherence in text summarization:

```
Evaluation Criteria:
Coherence (1-5) - the collective quality of all sentences. We align this dimension with the DUC quality question of structure and coherence whereby "the summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to sentence to a coherent body of information about a topic."
```

### Auto Chain-of-Thoughts for NLG Evaluation

The chain-of-thoughts (CoT) is a sequence of intermediate representations that are generated by the LLM during the text generation process. For evaluation tasks, some criteria need a more detailed evaluation instruction beyond the simple definition, and it is time-consuming to manually design such evaluation steps for each task. We find that LLM can generate such evaluation steps by itself. The CoT can provide more context and guidance for the LLM to evaluate the generated text, and can also help to explain the evaluation process and results.

For example, for evaluating coherence in text summarization, we add a line of "Evaluation Steps:" to the prompt and let LLM to generate the following CoT automatically:

```
1. Read the news article carefully and identify the main topic and key points.
2. Read the summary and compare it to the news article. Check if the summary covers the main topic and key points of the news article, and if it presents them in a clear and logical order.
3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.
```

### Scoring Function

The scoring function calls the LLM with the designed prompt, auto CoT, the input context and the target text that needs to be evaluated. Unlike GPTScore (Fu et al., 2023) which uses the conditional probability of generating the target text as an evaluation metric, G-EVAL directly performs the evaluation task with a form-filling paradigm.

However, we notice this direct scoring function has two issues:

1. **For some evaluation tasks, one digit usually dominates the distribution** of the scores, such as 3 for a 1-5 scale. This may lead to the low variance of the scores and the low correlation with human judgments.

2. **LLMs usually only output integer scores**, even when the prompt explicitly requests decimal values. This leads to many ties in evaluation scores which do not capture the subtle difference between generated texts.

To address these issues, we propose using the probabilities of output tokens from LLMs to normalize the scores and take their weighted summation as the final results. Formally, given a set of scores (like from 1 to 5) predefined in the prompt S = {s₁, s₂, ..., sₙ}, the probability of each score p(sᵢ) is calculated by the LLM, and the final score is:

**score = Σⁿᵢ₌₁ p(sᵢ) × sᵢ**

This method obtains more fine-grained, continuous scores that better reflect the quality and diversity of the generated texts.

## 3. Experiments

Following Zhong et al. (2022), we meta-evaluate our evaluator on three benchmarks, SummEval, Topical-Chat and QAGS, of two NLG tasks, summarization and dialogue response generation.

### 3.1 Implementation Details

We use OpenAI's GPT family as our LLMs, including GPT-3.5 (text-davinci-003) and GPT-4. For GPT-3.5, we set decoding temperature to 0 to increase the model's determinism. For GPT-4, as it does not support the output of token probabilities, we set 'n = 20, temperature = 1, top p = 1' to sample 20 times to estimate the token probabilities. We use G-EVAL-4 to indicate G-EVAL with GPT-4 as the backbone model, and G-EVAL-3.5 to indicate G-EVAL with GPT-3.5 as the backbone model.

### 3.2 Benchmarks

We adopt three meta-evaluation benchmarks to measure the correlation between G-EVAL and human judgments:

- **SummEval** (Fabbri et al., 2021): A benchmark that compares different evaluation methods for summarization. It gives human ratings for four aspects of each summary: fluency, coherence, consistency and relevance. It is built on the CNN/DailyMail dataset (Hermann et al., 2015).

- **Topical-Chat** (Mehri and Eskenazi, 2020): A testbed for meta-evaluating different evaluators on dialogue response generation systems that use knowledge. We follow (Zhong et al., 2022) to use its human ratings on four aspects: naturalness, coherence, engagingness and groundedness.

- **QAGS** (Wang et al., 2020): A benchmark for evaluating hallucinations in the summarization task. It aims to measure the consistency dimension of summaries on two different summarization datasets.

### 3.3 Baselines

We evaluate G-EVAL against various evaluators that achieved state-of-the-art performance:

- **BERTScore** (Zhang et al., 2019): Measures the similarity between two texts based on the contextualized embedding from BERT (Devlin et al., 2019).
- **MoverScore** (Zhao et al., 2019): Improves BERTScore by adding soft alignments and new aggregation methods to obtain a more robust similarity measure.
- **BARTScore** (Yuan et al., 2021): A unified evaluator which evaluate with the average likelihood of the pretrained encoder-decoder model, BART (Lewis et al., 2020).
- **FactCC and QAGS** (Krysciński et al., 2020; Wang et al., 2020): Two evaluators that measure the factual consistency of generated summaries.
- **USR** (Mehri and Eskenazi, 2020): An evaluator that assess dialogue response generation from different perspectives.
- **UniEval** (Zhong et al., 2022): A unified evaluator that can evaluate different aspects of text generation as QA tasks.
- **GPTScore** (Fu et al., 2023): A framework that evaluates texts with generative pre-training models like GPT-3.

## 4. Results

### 4.1 Summarization Results (SummEval)

| Metrics | Coherence | Consistency | Fluency | Relevance | AVG |
|---------|-----------|-------------|---------|-----------|-----|
| | ρ/τ | ρ/τ | ρ/τ | ρ/τ | ρ/τ |
| ROUGE-1 | 0.167/0.126 | 0.160/0.130 | 0.115/0.094 | 0.326/0.252 | 0.192/0.150 |
| ROUGE-2 | 0.184/0.139 | 0.187/0.155 | 0.159/0.128 | 0.290/0.219 | 0.205/0.161 |
| ROUGE-L | 0.128/0.099 | 0.115/0.092 | 0.105/0.084 | 0.311/0.237 | 0.165/0.128 |
| BERTScore | 0.284/0.211 | 0.110/0.090 | 0.193/0.158 | 0.312/0.243 | 0.225/0.175 |
| MOVERScore | 0.159/0.118 | 0.157/0.127 | 0.129/0.105 | 0.318/0.244 | 0.191/0.148 |
| BARTScore | 0.448/0.342 | 0.382/0.315 | 0.356/0.292 | 0.356/0.273 | 0.385/0.305 |
| UniEval | 0.575/0.442 | 0.446/0.371 | 0.449/0.371 | 0.426/0.325 | 0.474/0.377 |
| GPTScore | 0.434/– | 0.449/– | 0.403/– | 0.381/– | 0.417/– |
| G-EVAL-3.5 | 0.440/0.335 | 0.386/0.318 | 0.424/0.347 | 0.385/0.293 | 0.401/0.320 |
| - Probs | 0.359/0.313 | 0.361/0.344 | 0.339/0.323 | 0.327/0.288 | 0.346/0.317 |
| **G-EVAL-4** | **0.582/0.457** | **0.507/0.425** | **0.455/0.378** | **0.547/0.433** | **0.514/0.418** |
| - Probs | 0.560/0.472 | 0.501/0.459 | 0.438/0.408 | 0.511/0.444 | 0.502/0.446 |
| - CoT | 0.564/0.454 | 0.493/0.413 | 0.403/0.334 | 0.538/0.427 | 0.500/0.407 |

*Summary-level Spearman (ρ) and Kendall-Tau (τ) correlations on SummEval benchmark*

**Key Finding:** G-EVAL-4 achieved much higher human correspondence compared with G-EVAL-3.5 on both Spearman and Kendall-Tau correlation, indicating that the larger model size of GPT-4 is beneficial for summarization evaluation.

### 4.2 Dialogue Generation Results (Topical-Chat)

| Metrics | Naturalness | Coherence | Engagingness | Groundedness | AVG |
|---------|-------------|-----------|--------------|--------------|-----|
| | r/ρ | r/ρ | r/ρ | r/ρ | r/ρ |
| ROUGE-L | 0.176/0.146 | 0.193/0.203 | 0.295/0.300 | 0.310/0.327 | 0.243/0.244 |
| BLEU-4 | 0.180/0.175 | 0.131/0.235 | 0.232/0.316 | 0.213/0.310 | 0.189/0.259 |
| METEOR | 0.212/0.191 | 0.250/0.302 | 0.367/0.439 | 0.333/0.391 | 0.290/0.331 |
| BERTScore | 0.226/0.209 | 0.214/0.233 | 0.317/0.335 | 0.291/0.317 | 0.262/0.273 |
| USR | 0.337/0.325 | 0.416/0.377 | 0.456/0.465 | 0.222/0.447 | 0.358/0.403 |
| UniEval | 0.455/0.330 | 0.602/0.455 | 0.573/0.430 | 0.577/0.453 | 0.552/0.417 |
| **G-EVAL-3.5** | **0.532/0.539** | **0.519/0.544** | **0.660/0.691** | **0.586/0.567** | **0.574/0.585** |
| **G-EVAL-4** | **0.549/0.565** | **0.594/0.605** | **0.627/0.631** | **0.531/0.551** | **0.575/0.588** |

*Turn-level Spearman (ρ) and Kendall-Tau (τ) correlations on Topical-Chat benchmark*

**Key Finding:** G-EVAL substantially surpasses all previous state-of-the-art evaluators on the Topical-Chat benchmark. Notably, G-EVAL-3.5 can achieve similar results with G-EVAL-4, indicating that this benchmark is relatively easy for the G-EVAL model.

### 4.3 Hallucination Results (QAGS)

| Metrics | QAGS-CNN | QAGS-XSUM | Average |
|---------|----------|-----------|---------|
| | r/ρ/τ | r/ρ/τ | r/ρ/τ |
| ROUGE-2 | 0.459/0.418/0.333 | 0.097/0.083/0.068 | 0.278/0.250/0.200 |
| ROUGE-L | 0.357/0.324/0.254 | 0.024/-0.011/-0.009 | 0.190/0.156/0.122 |
| BERTScore | 0.576/0.505/0.399 | 0.024/0.008/0.006 | 0.300/0.256/0.202 |
| MoverScore | 0.414/0.347/0.271 | 0.054/0.044/0.036 | 0.234/0.195/0.153 |
| FactCC | 0.416/0.484/0.376 | 0.297/0.259/0.212 | 0.356/0.371/0.294 |
| QAGS | 0.545/–/– | 0.175/–/– | 0.375/–/– |
| BARTScore | 0.735/0.680/0.557 | 0.184/0.159/0.130 | 0.459/0.420/0.343 |
| CTC | 0.619/0.564/0.450 | 0.309/0.295/0.242 | 0.464/0.430/0.346 |
| UniEval | 0.682/0.662/0.532 | 0.461/0.488/0.399 | 0.571/0.575/0.465 |
| G-EVAL-3.5 | 0.477/0.516/0.410 | 0.211/0.406/0.343 | 0.344/0.461/0.377 |
| **G-EVAL-4** | **0.631/0.685/0.591** | **0.558/0.537/0.472** | **0.599/0.611/0.525** |

*Pearson (r), Spearman (ρ) and Kendall-Tau (τ) correlations on QAGS benchmark*

**Key Finding:** G-EVAL-4 outperforms all state-of-the-art evaluators on QAGS, with a large margin on QAGS-Xsum. G-EVAL-3.5 failed to perform well on this benchmark, indicating that the consistency aspect is sensitive to the LLM's capacity.

## 5. Analysis

### 5.1 LLM Bias Analysis: Will G-EVAL prefer LLM-based outputs?

One concern about using LLM as an evaluator is that it may prefer the outputs generated by the LLM itself, rather than the high-quality human-written texts. We conducted an experiment on the summarization task using the dataset from Zhang et al. (2023), where freelance writers wrote high-quality summaries for news articles, and annotators compared human-written summaries and LLM-generated summaries.

**Key Findings:**
- G-EVAL-4 assigns higher scores to human-written summaries when human judges also prefer human-written summaries
- G-EVAL-4 assigns lower scores when human judges prefer GPT-3.5 summaries
- **However, G-EVAL-4 always gives higher scores to GPT-3.5 summaries than human-written summaries, even when human judges prefer human-written summaries**

**Potential Reasons:**
1. NLG outputs from high-quality systems are naturally difficult to evaluate (inter-annotator agreement on judging human-written and LLM-generated summaries is very low, with Krippendorff's alpha at 0.07)
2. G-EVAL may have a bias towards LLM-generated summaries because the model could share the same concept of evaluation criteria during generation and evaluation

**Important Implication:** This bias could lead to self-reinforcement of LLMs if LLM-based metrics are used as reward signals for further tuning, potentially resulting in over-fitting to LLM evaluation criteria rather than true evaluation criteria.

### 5.2 Effect of Chain-of-Thoughts

Comparing G-EVAL with and without CoT on the SummEval benchmark shows that G-EVAL-4 with CoT has higher correlation than G-EVAL-4 without CoT on all dimensions, especially for fluency. This suggests that CoT can provide more context and guidance for the LLM to evaluate the generated text.

### 5.3 Effect of Probability Normalization

On Kendall-Tau correlation, G-EVAL-4 with probabilities is inferior to G-EVAL-4 without probabilities on SummEval. However, this is related to the calculation of Kendall-Tau correlation - direct scoring without probabilities can lead to many ties, resulting in artificially higher Kendall-Tau correlation that doesn't reflect true evaluation capacity. Probability normalization obtains more fine-grained, continuous scores that better capture subtle differences between generated texts, reflected by higher Spearman correlation.

### 5.4 Effect of Model Size

G-EVAL-4 has higher correlation than G-EVAL-3.5 on most dimensions and datasets, demonstrating that larger model size can improve G-EVAL performance, especially for challenging evaluation tasks such as consistency and relevance.

## 6. Related Work

### Ngram-based Metrics
- **BLEU** (Papineni et al., 2002): Geometric mean of modified n-gram precision with brevity penalty
- **ROUGE** (Lin, 2004): Recall-oriented metric measuring n-gram overlap
- **Limitations:** Fail to measure content quality or capture syntactic errors, low correlation with human judgments

### Embedding-based Metrics
- **WMD** (Kusner et al., 2015): Distance between texts based on word embeddings
- **BERTScore** (Zhang et al., 2019): Similarity based on contextualized BERT embeddings
- **MoverScore** (Zhao et al., 2019): Improved BERTScore with soft alignments

### Task-specific Evaluators
- **Summarization:** Assess consistency of generated summaries
- **Dialogue:** Assess coherence of generated responses
- **Limitation:** Not generalizable to other NLG tasks

### Unified Evaluators
- **UniEval** (Zhong et al., 2022): Evaluates different aspects as QA tasks
- **BARTScore** (Yuan et al., 2021): Uses BART likelihood for evaluation

### LLM-based Evaluators
- **GPTScore** (Fu et al., 2023): Uses GPT-3 generation probabilities
- **Preliminary surveys:** Wang et al. (2023) on ChatGPT as evaluator

## 7. Conclusion

We propose G-EVAL, a framework using LLM with chain-of-thoughts (CoT) to evaluate the quality of generated texts. Through extensive experiments on text summarization and dialogue generation, we show that G-EVAL can outperform state-of-the-art evaluators and achieve higher human correspondence. We also provide preliminary analysis on LLM-based evaluator behavior, highlighting the potential bias towards LLM-generated texts. This work aims to inspire more research on using LLMs for NLG evaluation while raising awareness of potential risks and challenges.

---

## Appendix: Example Prompts

### Evaluate Coherence in Summarization Task

```
You will be given one summary written for a news article.
Your task is to rate the summary on one metric.
Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.

Evaluation Criteria:
Coherence (1-5) - the collective quality of all sentences. We align this dimension with the DUC quality question of structure and coherence whereby "the summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to sentence to a coherent body of information about a topic."

Evaluation Steps:
1. Read the news article carefully and identify the main topic and key points.
2. Read the summary and compare it to the news article. Check if the summary covers the main topic and key points of the news article, and if it presents them in a clear and logical order.
3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.

Example:
Source Text:
{{Document}}
Summary:
{{Summary}}
Evaluation Form (scores ONLY):
- Coherence:
```

### Evaluate Engagingness in Dialogue Generation Task

```
You will be given a conversation between two individuals. You will then be given one potential response for the next turn in the conversation. The response concerns an interesting fact, which will be provided as well.
Your task is to rate the responses on one metric.
Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.

Evaluation Criteria:
Engagingness (1-3) Is the response dull/interesting?
- A score of 1 (dull) means that the response is generic and dull.
- A score of 2 (somewhat interesting) means the response is somewhat interesting and could engage you in the conversation (e.g., an opinion, thought)
- A score of 3 (interesting) means the response is very interesting or presents an interesting fact

Evaluation Steps:
1. Read the conversation, the corresponding fact and the response carefully.
2. Rate the response on a scale of 1-3 for engagingness, according to the criteria above.
3. Provide a brief explanation for your rating, referring to specific aspects of the response and the conversation.

Example:
Conversation History:
{{Document}}
Corresponding Fact:
{{Fact}}
Response:
{{Response}}
Evaluation Form (scores ONLY):
- Engagingness:
```

### Evaluate Hallucinations

```
Human Evaluation of Text Summarization Systems:
Factual Consistency: Does the summary untruthful or misleading facts that are not supported by the source text?

Source Text:
{{Document}}
Summary:
{{Summary}}

Does the summary contain factual inconsistency?
Answer:
```

---

**References:** [Complete bibliography with 40+ references preserved from original paper]